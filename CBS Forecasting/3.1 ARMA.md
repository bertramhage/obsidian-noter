---
Slides: "[[ARMA.pdf]]"
---
## Cycle models
We assume the cyclical component $C_t$ has a stationary mean and variance.

The $k$'th **autocovariance** of a time series $Y_t$ is the covariance of $Y_t$ with its lag $T_{t-k}$ written
$$
\gamma(k)=cov(Y_t, Y_{t-k})=E[(Y_t-\mu)(Y_{t-k}-\mu)]
$$
We also assume the cyclical component has a stationary covariance.

The $k$'th **autocorrelation** of a time series $Y_t$ is the correlation of $Y_t$ with $Y_{t-k}$ written
$$
\rho(k) = \frac{cov(Y_t, Y_{t-k})}{\sqrt{ var(Y_t)var(Y_{t-k}) }} = \frac{cov(Y_t, Y_{t-k})}{var(Y_t)}, \rho(k)\in[-1,1]
$$
A regression regressed on lagged values of itself is an autoregression.

## Moving average
**First order moving average MA(1)**:
$$y_t = \epsilon_t+\theta\epsilon_{t-1}, E[\epsilon_t]=0$$
where $\theta$ controls the degree of serial correlation.
$$
E[y_t] = E[\epsilon_t+\theta\epsilon_{t-1}]=E[\epsilon_t]+\theta E[\epsilon_{t-1}]=0
$$
$$
\begin{align}
var[y_t]&=var[\epsilon_t+\theta\epsilon_{t-1}]=var[\epsilon_t]+var[\theta\epsilon_{t-1}]+2cov[\epsilon_t,\theta\epsilon_{t-1}] \\
&=\sigma^2+\theta^2\sigma^2+0 = (1+\theta^2)\sigma^2
\end{align}
$$
The autocovariance for $k=1$ is
$$
\gamma(1) = \theta\sigma^2
$$
The autocovaraince for $k>1$ is
$$
\gamma(k) = 0
$$
## Autoregressive
The first-order autoregressive process $AR(1)$ is
$$
y_t = \beta y_{t-1}+\epsilon_t, E[\epsilon_t] = 0
$$
$$
\begin{align}
\rho(0)&=1 \\
\rho(1)&=\beta \rho(0)=\beta \\
p(2)&=\beta \rho(1)=\beta^2 \\
\vdots \\
\rho(k)=\beta^k
\end{align}
$$
If $\beta$ is small, the autocorrelations decay rapidly to zero with $k$
If $\beta$ is large the autocorrelations decay moderately

## ARMA
**ARMA(1,1)**:
$$
y_t = \beta y_{t-1}+\epsilon_t+\theta\epsilon_{t-1}, E[\epsilon_t]=0
$$
Akaike Information Criterion (AIC)
$$
AIC = \exp \left\{ \frac{2k}{T} \right\} \frac{\sum_{t=1}^{T} \hat{\epsilon}^2_t \, }{T}
$$
where $k$ is the number of estimated parameters.

**ARMA(p,q)**
We can estimate as many ARMA(p,q) models as we can and choose the one with the smallest AIC.