---
Uge: "11"
Dato: 17/4/2024
Slides: "[[Week 11 slides  - 2024_v1.pdf]]"
---
## Logistic regression
Logistic regression is used to model binary random variables
In other words we model $P(Y=0)$ and $P(Y=1)=1-P(Y=0)$.

Using the notation $\pi(x)=P(Y=1|X=x)=P(Y=1|x)$ then the logistic regression is defined by
$$
\pi(x)=\frac{\exp(\beta'x)}{1+\exp(\beta'x)}\text{ or }\ln\left( \frac{\pi}{1-\pi} \right)=\beta'x
$$
where $\beta$ are the parametres that we would like to estimate.

>[!example]+ Accident data
>We have a data set with 3643 observations of which 723 have been involved in an accident and the remaining 2920 have not. In the data, we have the variables: gender, age group and blood alcohol concentration (BAC). What types are the variables? Nominally, ordinally, interval, or ratio scaled?
>
>To investigate how alcohol affects the risk of accident, we apply the model:
>$$
>\ln\left( \frac{\pi}{1-\pi} \right)=\beta_0 + \beta_{age24}x_{age18-24}+\beta_{age34}x_{age25-34}+\beta_{age49}x_{age35-49}+\beta_{fem}x_{fem}+\beta_{bac}x_{bac}
>$$
>where $x_{fem}$ is a dummy for female, $x_{age18-24}, x_{age25-34}, x_{age35-49}$ are dummies for age groups $x_{bac}$ is the alcohol percentage.
>**Result:**
>![[SkÃ¦rmbillede 2024-04-24 kl. 13.18.33.png]]
>+ The probability for accidents decreases with age everything else equal
>+ The probability rises with the alcohol percentage everything else equal
>+ The probability is higher for females everything else equal

#### Predictions
Predictions using the model can be done using the expression
$$
P(y=1|x)=\pi(x)=\frac{\exp(\beta'x)}{1+\exp(\beta'x)},
$$
which is only a function of the $x$'s once the $\beta$'s are estimated.

>[!example]+ Accidents continued
>Having estimated the parameters such that:
>$$
>\ln\left( \frac{\pi}{1-\pi} \right)=-2.48 + 2.17x_{age18-24}+1.05x_{age25-34}+0.42x_{age35-49}+0.34x_{fem}+2.99x_{bac}
>$$
>we calculate $E(Y|x)=P(Y=1|x)$ by
>$$
>P(Y=1|x)=\frac{\exp(-2.48+2.17\cdot 1)}{1+\exp(-2.48+2.17\cdot 1)}=0.42
>$$

#### Judging parameters
It is not a good idea to interpret the $\beta$s directly

Therefore other ways include
+ Marginal effects
$$
P(y=1|x)=\pi(x)=\frac{\exp(\beta'x)}{1+\exp(\beta'x)}\text{ and } \frac{\partial \pi}{\partial x_k}=1(1-\pi)\pi \beta_k
$$
+ Elasticities
$$
E=\frac{\partial \pi}{\partial x_k} \frac{x_k}{\pi}=(1-\pi)x_k\beta_k
$$
+ Odds ratios
	Suppose we calculate the log odds for a male and female, respectively, with the same remaining values and take the difference
$$
\ln\left( \frac{\pi_f}{1-\pi_f} \right)-\ln\left( \frac{\pi_m}{1-\pi_m} \right)=\beta_{fem}\text{ i.e. odds ratio = } \frac{\frac{\pi_f}{1-\pi_f}}{\frac{\pi_m}{1-\pi_m}}=\exp(\beta_{fem})
$$
+ Suppose an example where observations are either treatment or control. In the control group there is a $0.50$ risk of death while in the treatment group there is a $0.25$ risk of death.
+ Here the relative risk of dying with treatment versus without is $0.25/0.50 = 0.50$. On the other hand the odds ratio can be found to be $0.33$.

## Maximum likelihood estimation for logistic regression

Maximum likelihood estimation for logistic regression is given by
$$
\hat{\beta} = argmax_\beta LL(\beta)
$$
where
$$
LL(\beta)=\sum_n \ln(P(y_n|x_n, \beta))
$$
MLE is consistent (but biased) and asymptotically normal.

#### Comparison
Using the maximum (log)likelihood value, $LL(\hat{\beta})$, we can compare two models estimated on the same data by the likelihood ratio (LR) test.
$$
LR = -2\cdot (LL(\hat{\beta}_1)-LL(\hat{\beta}_2))\sim \chi^2_f.
$$
## Overall model evaluation

**McFaddens $\rho^2$**
$$
\rho^2=1-\frac{LL(\hat{\beta})}{LL(0)}\text{ and } \hat{\rho}^2=1- \frac{LL(\hat{\beta})-K}{LL(0)}
$$
where $K$ is the number of parameters in the model. 

## Predicting an outcome 
In a confusion matrix, we used the standard way to predict a likely outcome for each observation, i.e. for a given threshold T
$$
\hat{y}_n=\begin{cases}
0 & ,if P(y_n=1|x_n)<T \\
1 & , if P(y_n=1|x_n)>T
\end{cases}
$$
However as the example showed, it might not be the most useful approach that gives the lowest total error rate. Another approach is to make predictions that follow the distribution captured by the model, i.e. draw a random number $s\sim U[0,1]$ and calculate
$$
\hat{y}_n=\begin{cases}
0 & ,if P(y_n=1|x_n)<s \\
1 & , if P(y_n=1|x_n)>s
\end{cases}
$$
## Generalized linear models
We use logistic regression if we need to model a binary variable $Y$.

For a binary random varaible we have the result
$$
E(Y)=1\cdot P(Y=1)+0\cdot P(Y=0)=P(Y=1)
$$
So when we model $P(Y=1)$ we actually model $E(Y)$, hence $Y=P(Y=1)+\epsilon$

This makes logistic regression similar to linear regression where we also model the expectation. The difference is that $Y$ is Bernouilli distributed, that $E(Y)$ only can take on values between 0 and 1, and that $E(Y)$ and the residual are correlated.