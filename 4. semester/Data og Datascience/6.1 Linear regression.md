---
Uge: Uge 9 - 3/11/23
Dato: 3/4/2024
Slides: "[[Week 9 slides  - 2024_v0.pdf]]"
---
## Prediction

Having predicted $f(X)$ the common way to predict $\hat{Y}$ is
$$
\hat{Y} = \hat{f}(X)
$$
The MSE depends on two quantities:
- The reducible error, which can be reduced if we choose a better model $\hat{f}$
- The irreducible error, which cannot be lowered by modelling since it is the uncertainty inherent in the context.

We have
$$
E(Y-\hat{Y})^2=[f(X)-\hat{f}(x)]^2+Var(\epsilon)
$$

## Inference

Given a model
$$
Y = f(X;\beta)+\epsilon
$$

Inference is the focus to estimate the specific $\beta$ parameters

#### Trade-off prediction accuracy vs interpretability
![[SkÃ¦rmbillede 2024-04-03 kl. 13.18.25.png|500]]
## Linear model

>[!info] Linear regression
>The linear regression model is 
>$$
>Y_i = E(Y_i|X_i) + \epsilon_i=\beta_0+\beta_1X_{1i} + \cdots+\beta_pX_{pi} + \epsilon_i=X_i\beta+\epsilon_i
>$$
>where $Y_i$ is the dependent variable, and $X_i$ is a a vector of independant variables.

#### Recoding of discrete variables
Discrete variables can be entered either as they are (if the have a natural ordering), or recode them using **dummy variables**.

###### Recoding with dummy variables
- Choose a base level, one of the variables, often the mode
- Set up binary dummy variables for the rest of the variables

## T test and p values

>[!info] $t$ statistics and $p$ value
>The **t statistics** are the test statistics for the hypothesis that each parameter is equal to zero, i.e.
>$$
>H_0:\beta_k=\beta_0, H_1:\beta_k\neq \beta_0
>$$
>The test statistic is calculated based on the formula $Z^*=\frac{\hat{\beta_k}-\beta_0}{\hat{s}_k}$ with $\beta_0=0$.
>
>The p value is the probability that the test statistic, e.g. $Z^*$ could have been more extreme than the value calculated based on the data, i.e.
>$$
>p=P(|Z^*|>Z_0)
>$$
#### Significance
To test significance at the $\alpha$ level, we calculate $Z^*=\frac{\hat{\beta_k}-\beta_0}{\hat{s}_k}\sim_\alpha N(0,1)$ and compare this value to the critical values at the $\alpha$ level for the normal distribution e.g. -1.96 and +1.96 at the 5% level.

## Evaluation

#### $R^2$
+ A measure between 0 and 1
+ Only to be compared between models explaining the same thing
+ $R^2=0.2$ can be good in one context while $R^2=0.9$ might be bad in another context
+ Adjusted $R^2$:
	+ $R^2_{adjusted}=1-\frac{(n-1)RSS}{(n-p)TSS'}$ where $n$ is no. obs. and $p$ is no. params.

#### Prediction certainty
Three sorts of uncertainties exists in predictions

1. Model approximation bias (reducible error)
	$\hat{Y}$ is an approximation of $f(X)$. This uncertainty can be quantified by a confidence interval around $\hat{Y}$. 
2. Model bias (reducible error)
	The linear model is an approximation. We might need to choose a more flexible non-linear model.
3. Noise (irreducible error)
	Even with the correct model there will be uncertainty due to the irreducible error. This can be captured by a prediction interval.

## Elasticities

Effect of the change in a independent variable on the dependent variable

>[!info] Marginal effect
>For a linear model 
>$$
>Y_i = \beta_0 + \beta_1X_{1i}+\cdots+\beta_pX_{pi}+\epsilon_i
>$$
>we can find the marginal effect of $X_k$ on $Y$ as:
>$$
>\frac{\partial Y}{\partial X_k}=\beta_k.
>$$

Elasticity disregards units

>[!info] Elasticity
>For a linear model 
>$$
>Y_i = \beta_0 + \beta_1X_{1i}+\cdots+\beta_pX_{pi}+\epsilon_i
>$$
>we can find the elasticity as
>$$
>E_{Y,X_k} = \frac{\partial Y}{\partial X_k} \frac{X_{k}}{Y}=\beta_k \frac{X_k}{Y}
>$$

## Model building diagram
![[Pasted image 20240410131557.png|600]]

## OLS assumptions

There are six assumptions that are useful for the linear regression model:

1. Functional form
$$
Y_i=X_i \beta+\varepsilon_i
$$
2. Zero mean of disturbances
$$
E\left(\varepsilon_i\right)=0
$$
3. Homoscedasticity of disturbances
$$
\operatorname{VAR}\left(\varepsilon_i\right)=\sigma^2
$$
4. Nonautocorrelation of disturbances
$$
\operatorname{cov}\left(\varepsilon_i, \varepsilon_j\right)=0
$$
5. Uncorrelatedness of regression and disturbances
$$
\operatorname{cov}\left(X_i, \varepsilon_j\right)=0
$$
6. Normality of disturbances
$$
\varepsilon_i \sim N\left(0, \sigma^2\right)
$$
The conditions can be reformulated when $X$ is stochastic. Then we think of the modelling as conditional on $\mathrm{X}$. e.q. the condition $E\left(\varepsilon_i \mid X_i\right)=0$ implies conditions 2 and 5 .