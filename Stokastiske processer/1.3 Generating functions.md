---
Dato: 8/9/2024
Slides: "[[week2.pdf]]"
Pensum: 3.9-3.9.2
Teaching Notes: "[[lesson2.pdf]]"
---
Given a probability mass function
$$
Pr\{\xi=k\}=p_k, \text{ for }k=0,1,\dots
$$
the generating function $\phi(s)$ associated with the random variable $\xi$ is defined by
$$
\phi(s)=E[s^\xi]=\sum_{k=0}^{\infty} p_ks^k \, \text{ for } 0≤s≤1.
$$
The relation that expresses the probability mass function $p_k$ in terms of the generating function is
$$
p_k=\left.\frac{1}{k!} \frac{\mathrm{d}^k \phi(s)}{\mathrm{d} s^k}\right|_{s=0}
$$
## Properties
#### Expected value
$$
\left. \frac{\mathrm{d}\phi(s)}{\mathrm{d}s}\right|_{s=1}=\left.\sum_{k=1}^{\infty} p_kks^{k-1}\right|_{s=1}=E[\xi] \,
$$
#### Variance
$$
\left. \frac{\mathrm{d}^2\phi(s)}{\mathrm{d}s^2}\right|_{s=1} = \left. \sum_{k=2}^{\infty} p_kk(k-1)s^{k-2}\right|_{s=1} = E[\xi(\xi-1)] \, 
$$
$$
Var[\xi] = \phi''(1)+\phi'(1)-(\phi'(1))^2
$$
## Sum of random variables
>[!info]+ Prerequisite
>Recall that for iid.  random variables
>$$
>S_n = X_1+X_2+\dots+X_n =\sum_{i=1}^{n} X_i \text{ with } p_x=P(X_i=x) \, 
>$$
>for $n=2, S_2=X_1+X_2$:
>$$
>Pr\{S_2=x\}=\sum_{i=0}^{x} p_ip_{x-i} \, 
>$$

If $X$ and $Y$ are independent then
$$
\phi_{X+Y}(s)=\phi_X(s)\phi_Y(s)
$$
**Example with poisson distribution**
$$
X\sim P(\lambda) \text{ } Y\sim P(\mu) \text{ } Z=X+Y
$$
$$
\phi_x(s)=e^{-\lambda(1-s)} \text{ } \phi_Y(s)=e^{-\mu(1-s)} \text{ } \left( Pr\left\{ X=x \right\} =p_x=\frac{\lambda^x}{x!}e^{-\lambda} \right)
$$
$$
\phi_Z(s)=\phi_X(s)\phi_Y(s)=e^{-\lambda(1-s)}e^{-\mu(1-s)}=e^{-(\lambda+\mu)(1-s)}
$$
$$
\implies Z\sim P(\lambda+\mu)
$$
