---
Pensum: 5.1.1-5.1.2, 5.2-5.4, (5.5), 5.6
Slides: "[[week4.pdf]]"
Dato: 24/9/2024
---
## Poisson distribution
The poisson distribution with parameter $\mu>0$ is
$$
p_k = \frac{e^{-\mu}\mu^k}{k!}\text{ for }k=0,1,\dots
$$
For a random variable $X\sim Pois(\mu)$
$$
E[X]=Var[X]=\mu
$$
**Theorem 5.1**: $X\sim Pois(\mu), Y\sim Pois(v)\implies X+Y\sim Pois(\mu +v)$
**Theorem 5.2**: $N\sim Pois(\mu), M\sim Binom(N,p)\implies M\sim Pois(\mu p)$

## Poisson process as independent increments
>[!info] **Definition**
>A Poisson process of intensity, or rate, $\lambda>0$ is an integer-valued stochastic process $\left\{ X(t);t≥0 \right\}$ for which
>1. for any time points $t_0=0<t_1<t_2<\dots<t_n$ the process increments
>$$X(t_1)-X(t_0), X(t_2)-X(t_1), \dots, X(t_n)-X(t_n-1)$$
>are independent random variables.
>
>2. for $s≥0$ and $t>0$ the random variable $X(s+t)-X(s)$ has the Poisson distribution 
>$$Pr\left\{ X(s+t)-X(s) =k\right\}=\frac{(\lambda t)^ke^{-\lambda t}}{k!}\text{ for }k=0,1,\dots$$
>
>3. $X(0)=0$

We have
$$
E[X(t)]=\lambda t \text{ and }Var[X(t)]=\sigma_{X(t)}^2=\lambda t.
$$
## As intensity/infinitesimal probabilities
#### The law of rare events
Consider a large number of $N$ independent Bernoulli trials with probability $p$ of success on each trial, where $p$ is small. Let $X_{N,p}$ denote the total number of successes in $N$ trials, $X_{N,p}\sim Binom(N,p)$. Then as $N\to \infty$ and $p\to_0$ such that $Np=\mu>0$ where $\mu$ is constant:
$$
\begin{align}
&\lim_{ N \to \infty, p\to_0 } \left( Pr\left\{ X_{N,p}=k \right\} = \frac{N!}{k!(N-k)!}p^k(1-p)^{N-k}   \right) \\
= & Pr\left\{ X_\mu =k\right\}= \frac{e^{-\mu}\mu^k}{k!}
\end{align}
$$
for $k=0,1,\dots$

Thus, where a certain event may occur in any of a large number of possibilities, but where the probability that the event does occur in any given possibility is small, then the total number of events that do happen should follow, approximately, the Poisson distribution.

This is generalized to cases where the probability for each Bernoulli experiment is not equal:

>[!info] Theorem 5.3
>Let $\epsilon_1, \epsilon_2,\dots$ be independent Bernoulli random variables, where
>$$
>Pr\left\{ \epsilon_i=1 \right\} =p_i \text{ and } Pr\left\{ \epsilon_i=0 \right\} =1-p_i
>$$
>and let $S_n=\epsilon_1+\cdots+\epsilon_n$. The exact probabilities for $S_n$ and Poisson probabilities with $\mu=p_1+\cdots+p_n$ differ by at most
>$$
>\left| Pr\left\{ S_n=k \right\} -\frac{\mu^ke^{-\mu}}{k!} \right| ≤\sum_{i=1}^{n} p_i^2. \, 
>$$

Let $N(]a,b])$ denote the number of events that occur during the interval $]a,b]$.
We make the following postulates:
1. The number of events happening in disjoint intervals are independent random variables
$$
N(]t_0,t_1]), N(]t_1,t_2]), \dots, N(]t_{m-1}, t_m])
$$
are **independent**, for every integer $m=2,3,\dots$ and time points of succesive events $t_0=0<t_1<t_2<\dots<t_m$.
2. For any time $t$ and $h>0$ the probability distribution of $N(]t,t+h])$ depends only on the interval length $h$ and not the time $t$.
3. $\exists \lambda>0(Pr\left\{ N]t,t+h]≥1 \right\}=\lambda h+o(h)\text{ as }h\downarrow0)$
4. $Pr\left\{ N(]t,t+h])≥2 \right\}=o(h), h\downarrow0$

>[!info] Definition
>Let $N(]s,t])$ be a random variable counting the number of events occuring in an interval $]s,t]$. Then, $N(]s,t])$ is a Poisson process of intensity $\lambda>0$ if
>1. for every $m=2,3,\dots$ and distinct time points $t_0=0<t_1<t_2<\dots<t_m$ the random variables
>$$
>N(]t_0, t_1]), N(]t_1, t_2]), \dots, N(]t_{m-1}, t_m])
>$$
>are independant, and
>2. for any time $s<t$ the random variable $N(]s,t])$ has the Poisson distribution
>$$
>Pr\left\{ N(]s,t])=k \right\} =\frac{[\lambda(t-s)]^ke^{-\lambda(t-s)}}{k!}, k=0,1,\dots
>$$


## As a sequence of independent exponential intervals

**Sojourn time** $S_n$ is the difference $S_n=W_{n+1}-W_n$, where $W_n$ is the time of occurrence of the $n$th event, and measures the duration that the Poisson process sojourns in state $n$.

![[Pasted image 20241024122504.png|400]]

>[!info] Theorem 5.4 Distribution of waiting times
>The waiting time $W_n$ has the gamma distribution whose PDF is
>$$
>f_{W_n}(t)=\frac{\lambda^nt^{n-1}}{(n-1)!}e^{-\lambda t}, n=1,2,\dots,t≥0
>$$
>In particular, $W_1$, is exponentially distributed:
>$$
>f_{W_1}(t)=\lambda e^{-\lambda t},t≥0
>$$

>[!info] Theorem 5.5 Distribution of sojourn times
>The sojourn times $S_0, S_1, \dots, S_{n-1}$ are independent random variables, each having the exponential PDF
>$$
>f_{S_{k}}(s)=\lambda e^{-\lambda s}, s≥0
>$$

>[!info] Theorem 5.6 Binomial distribution for subintervals
>Let $\left\{ X(t) \right\}$ be a Poisson process of rate $\lambda>0$. Then for $0<u<t$ and $0≤k≤n$,
>$$
>Pr\left\{ X(u)=k|X(t)=n \right\} =\frac{n!}{k!(n-k)!}\left( \frac{u}{t} \right)^k\left( 1-\frac{u}{t} \right)^{n-k}.
>$$

## The uniform distribution and poisson process
>[!info] Theorem 5.7
>Let $W_1, W_2,\dots$ be the occurrence times in a Poisson process of rate $\lambda>0$. Conditioned on $X(t)=n$, the random variables $W_1,W_2,\dots,W_n$ have the joint PDF
>$$
>f_{W_1,\dots,W_n|X(t)=n}(w_1,\dots,w_n) = n!t^{-t}\text{ for }0<w_1<\dots<w_n≤t.
>$$

## Compound and marked Poisson process
For a Poisson process $X(t)$ of rate $\lambda>0$ suppose that each event has associated with it a random variable whose value is independent, independent of the Poisson process with the common distribution function
$$
G(y)=Pr\left\{ Y_k≤y \right\} 
$$
A **compound Poisson process** is the cumulative value process
$$
Z(t)=\sum_{k=1}^{X(t)} Y_k\text{ for t≥0} \, .
$$
A **marked Poisson process** is the sequence of pairs $(W_1, Y_1), (W_2, Y_2),\dots$ where $W_1,W_2,\dots$ are the waiting times in the Poisson process.

For a compound Poisson process $Z(t)=\sum_{k=1}^{x(t)} Y_k \,$ where $X(t)\sim Pois(\lambda)$, $\mu=E[Y_1]$ and $v^2=Var[Y_1]$ we have 
$$
E[Z(t)]=\lambda \mu t \text{ and } Var[Z(t)]=\lambda(v^2+\mu^2)t. 
$$
