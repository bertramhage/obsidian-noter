---
Dato: 8/9/2024
Slides: "[[week2.pdf]]"
Pensum: Pinsky & Karlin 3.6 up to 3.6.1 (3.7, 2.3, 3.9.1-3.9.2, 3.8)
Teaching Notes: "[[lesson2.pdf]]"
---
## "Gamblers Ruin"
Given a probability matrix of $N+1$ states
$$
P = \left|\left|\begin{matrix}1 & 0 & 0 & 0 &  \dots &  0 \\
q  & 0 & p & 0 & \dots & 0  \\
0 & q & 0 & p & \dots & 0 \\
\vdots & \vdots & \vdots & \vdots &  & \vdots \\
0 & 0 & 0 & 0 & \dots & 1\\
\end{matrix}\right|\right|
$$
we set 
$$
T = min\{n≥0;X_n=0\text{ or }X_n=N\}
$$
Example:
![[Pasted image 20240930134513.png|400]]
#### Absorption probabilities
The probability of this event from the initial state $k$ is 
$$
u_k = Pr\left\{ X_T = 0|X_0=k \right\}.
$$
See (125-126) for proof that
$$
u_k = \begin{cases}
1-(k/N)=(N-k)/N & \text{when } p=q=\frac{1}{2}, \\
1-\frac{1-(q/p)^k}{1-(q/p)^N}=\frac{(q/p)^k-(q/p)^N}{1-(q/p)^N}& \text{when }p\neq q.
\end{cases}
$$
#### Expected time to absorption
$$
v_i = E[T|X_0=i]
$$
Se (127-128) for proof that
$$
v_k = k(N-k), k=0,1,\dots,N.
$$
## Direct calculation as opposed to first step analysis
Consider the matrix from [[1.1 Markov chains]] 
$$
P = \left|\left|\begin{matrix}
Q & R \\
0 & I
\end{matrix}\right|\right|
$$
with transient states $0, 1, \dots, r-1$ and absorbing states $r, \dots, N$.

$$
P^n = \left|\left|\begin{matrix}
Q^n & (I+Q+\dots+Q^{n-1})R \\
0 & I
\end{matrix}\right|\right|.
$$
Let $W_{ij}^{(n)}$ be the mean number of visits to state $j$ up to stage $n$ for a Markov chain stating in state $i$:
$$
W_{ij}^{(n)}=E\left[ \sum_{l=0}^{n} \mathbf{1}\left\{ X_l=j \right\} |X_0=i \,  \right]
$$
See (140-141) for proof that
$$
W_{ij}^{(n)}=\sum_{l=0}^{n} P_{ij}^{(l)} \, 
$$
matrix-notation:
$$
\begin{align}
W^{(n)}&=I+Q+Q^{2}+\dots+Q^n \\
& = I+QW^{(n-1)}.
\end{align}
$$
**For the limit:**
$$
\begin{align}
W_{ij}&=\lim_{ n \to \infty } W_{ij}^{(n)}=E[\text{Total visits to $j$}|X_0=i], 0≤i, j<r,\\
&=\delta_{ij}+\sum_{l=0}^{r-1} P_{il}W_{lj}, \text{ for }i,j=0,\dots,r-1 \,.
\end{align}
$$
$$
W = I+Q+Q^{2}+\dots=\sum_{i=0}^{\infty} Q^i=I+QW \, 
$$
$$
\implies W = (I-Q)^-1
$$
#### Absorption time
$$
\sum_{n=0}^{T-1} \sum_{j=0}^{r} \mathbf{1}(X_n=j)=\sum_{n=0}^{T-1} 1=T \,  \,  \, 
$$
Thus
$$
\begin{align}
E[T|X_0=i]&=E\left[ \sum_{j=0}^{r} \sum_{n=0}^{T-1} \mathbf{1}(X_n=j|X_0=1) \,  \right] \\
&=\sum_{j=0}^{r} E\left[ \sum_{n=0}^{T-1} \mathbf{1}(X_n=j|X_0=i) \,  \right] \,  \\
&=\sum_{j=0}^{r} W_{ij} \, 
\end{align}
$$
matrix-notation:
$$
v = W1
$$
where 1 is a column vector of ones.
#### Absorption probabilities
$$
\begin{align}
U_{ij}^{(n)} = Pr\left\{ T≤n, X_T=j|X_0=i \right\}  \\
U^{(1)} = R=IR \\
U^{(2)}=IR+QR \\
U^{(n)}=(I+Q+\dots+Q^{(n-1)})R=W^{(n-1)}R \\
\implies U=WR
\end{align}
$$

